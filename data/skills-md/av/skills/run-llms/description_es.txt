Guía para configurar y ejecutar LLM locales usando [Harbor] (https://github.com/av/harbor). Úselo cuando el usuario desee ejecutar LLM localmente, configurar Ollama, Open WebUI, llama.cpp, vLLM o servicios de IA locales similares. Cubre la configuración completa desde los requisitos previos de Docker hasta la ejecución de modelos, configuración, perfiles, túneles y funciones avanzadas.
