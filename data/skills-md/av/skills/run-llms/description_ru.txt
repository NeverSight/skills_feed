Руководство по настройке и запуску локальных LLM с использованием [Harbor] (https://github.com/av/harbor). Используйте, когда пользователь хочет запускать LLM локально, настроить Ollama, Open WebUI, llama.cpp, vLLM или аналогичные локальные службы AI. Охватывает полную настройку, начиная с предварительных требований Docker и заканчивая запуском моделей, конфигурацией, профилями, туннелями и расширенными функциями.
