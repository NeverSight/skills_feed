Guide pour configurer et exécuter des LLM locaux à l'aide de [Harbor](https://github.com/av/harbor). À utiliser lorsque l'utilisateur souhaite exécuter des LLM localement, configurer Ollama, Open WebUI, lama.cpp, vLLM ou des services d'IA locaux similaires. Couvre la configuration complète à partir des prérequis de Docker via l'exécution de modèles, de configurations, de profils, de tunnels et de fonctionnalités avancées.
