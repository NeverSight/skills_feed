Guida per la configurazione e l'esecuzione di LLM locali utilizzando [Harbor](https://github.com/av/harbor). Da utilizzare quando l'utente desidera eseguire LLM localmente, configurare Ollama, Open WebUI, llama.cpp, vLLM o servizi AI locali simili. Copre la configurazione completa dai prerequisiti Docker fino all'esecuzione di modelli, configurazione, profili, tunnel e funzionalit√† avanzate.
