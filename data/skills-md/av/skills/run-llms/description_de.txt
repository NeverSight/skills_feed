Leitfaden zum Einrichten und Ausführen lokaler LLMs mit [Harbor](https://github.com/av/harbor). Verwenden Sie diese Option, wenn der Benutzer LLMs lokal ausführen, Ollama, Open WebUI, llama.cpp, vLLM oder ähnliche lokale KI-Dienste einrichten möchte. Deckt die vollständige Einrichtung von den Docker-Voraussetzungen bis hin zu laufenden Modellen, Konfigurationen, Profilen, Tunneln und erweiterten Funktionen ab.
