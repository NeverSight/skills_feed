使用 [Harbor](https://github.com/av/harbor) 设置和运行本地 LLM 的指南。当用户想要在本地运行 LLM、设置 Ollama、Open WebUI、llama.cpp、vLLM 或类似的本地 AI 服务时使用。涵盖从 Docker 先决条件到运行模型、配置、配置文件、隧道和高级功能的完整设置。
