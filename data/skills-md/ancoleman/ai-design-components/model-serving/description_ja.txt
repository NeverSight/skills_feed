推論のための LLM および ML モデルのデプロイメント。実稼働環境でモデルを提供する場合、AI API を構築する場合、または推論を最適化する場合に使用します。 vLLM (LLM サービング)、TensorRT-LLM (GPU 最適化)、Ollama (ローカル)、BentoML (ML デプロイメント)、Triton (マルチモデル)、LangChain (オーケストレーション)、LlamaIndex (RAG)、ストリーミング パターンをカバーします。
