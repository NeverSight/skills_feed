Déploiement de modèles LLM et ML pour l'inférence. À utiliser lors de la diffusion de modèles en production, de la création d'API d'IA ou de l'optimisation de l'inférence. Couvre vLLM (service LLM), TensorRT-LLM (optimisation GPU), Ollama (local), BentoML (déploiement ML), Triton (multimodèle), LangChain (orchestration), LlamaIndex (RAG) et les modèles de streaming.
