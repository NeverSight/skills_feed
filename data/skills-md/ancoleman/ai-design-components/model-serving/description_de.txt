Bereitstellung von LLM- und ML-Modellen f√ºr Inferenz. Verwenden Sie es, wenn Sie Modelle in der Produktion bereitstellen, KI-APIs erstellen oder Inferenz optimieren. Deckt vLLM (LLM-Bereitstellung), TensorRT-LLM (GPU-Optimierung), Ollama (lokal), BentoML (ML-Bereitstellung), Triton (Multi-Modell), LangChain (Orchestrierung), LlamaIndex (RAG) und Streaming-Muster ab.
