Implementación de modelos LLM y ML para inferencia. Úselo cuando proporcione modelos en producción, cree API de IA u optimice la inferencia. Cubre vLLM (servicio LLM), TensorRT-LLM (optimización de GPU), Ollama (local), BentoML (implementación de ML), Triton (multimodelo), LangChain (orquestación), LlamaIndex (RAG) y patrones de transmisión.
