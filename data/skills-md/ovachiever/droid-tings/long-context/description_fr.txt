Étendez les fenêtres contextuelles des modèles de transformateur à l'aide des techniques RoPE, YaRN, ALiBi et d'interpolation de position. À utiliser lors du traitement de documents longs (32 000 à 128 000+ jetons), de l'extension de modèles pré-entraînés au-delà des limites du contexte d'origine ou de la mise en œuvre d'encodages de position efficaces. Couvre les intégrations rotatives, les biais d'attention, les méthodes d'interpolation et les stratégies d'extrapolation pour les LLM.
