최대 300줄의 교육용 GPT 구현입니다. OpenWebText에서 GPT-2(124M)를 재현합니다. 변환기 학습을 위한 깔끔하고 해킹 가능한 코드입니다. 안드레이 카르파티(Andrej Karpathy) 지음. GPT 아키텍처를 처음부터 이해하는 데 적합합니다. 셰익스피어(CPU) 또는 OpenWebText(다중 GPU)에서 훈련합니다.
