تنفيذ GPT التعليمي في 300 سطر تقريبًا. يعيد إنتاج GPT-2 (124M) على OpenWebText. كود نظيف وقابل للاختراق لمحولات التعلم. بقلم أندريه كارباثي. مثالية لفهم بنية GPT من الصفر. تدرب على شكسبير (وحدة المعالجة المركزية) أو OpenWebText (وحدة معالجة الرسومات المتعددة).
