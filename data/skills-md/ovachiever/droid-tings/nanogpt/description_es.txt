Implementación educativa de GPT en ~300 líneas. Reproduce GPT-2 (124M) en OpenWebText. Código limpio y pirateable para aprender transformadores. Por Andrej Karpathy. Perfecto para comprender la arquitectura GPT desde cero. Entrene en Shakespeare (CPU) u OpenWebText (multi-GPU).
