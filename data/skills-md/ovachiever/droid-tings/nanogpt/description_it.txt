Implementazione GPT educativa in ~300 righe. Riproduce GPT-2 (124M) su OpenWebText. Codice pulito e hackerabile per i trasformatori di apprendimento. Di Andrej Karpathy. Perfetto per comprendere da zero l'architettura GPT. Allenati su Shakespeare (CPU) o OpenWebText (multi-GPU).
