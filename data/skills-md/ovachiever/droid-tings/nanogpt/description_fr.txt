Implémentation GPT pédagogique en environ 300 lignes. Reproduit GPT-2 (124 M) sur OpenWebText. Code propre et piratable pour apprendre les transformateurs. Par Andrej Karpathy. Parfait pour comprendre l’architecture GPT à partir de zéro. Entraînez-vous sur Shakespeare (CPU) ou OpenWebText (multi-GPU).
