教育用 GPT の実装は約 300 行で行われます。 OpenWebText上でGPT-2(124M)を再現します。トランスフォーマーを学習するためのクリーンでハッキング可能なコード。アンドレイ・カルパシー著。 GPT アーキテクチャをゼロから理解するのに最適です。 Shakespeare (CPU) または OpenWebText (マルチ GPU) でトレーニングします。
