Образовательная реализация GPT примерно в 300 строк. Воспроизводит GPT-2 (124M) в OpenWebText. Чистый, взломанный код для обучения трансформеров. Автор Андрей Карпаты. Идеально подходит для понимания архитектуры GPT с нуля. Тренируйтесь на Shakespeare (ЦП) или OpenWebText (многочиповый процессор).
