教育 GPT 实现约 300 行。在 OpenWebText 上复制 GPT-2 (124M)。用于学习 Transformer 的干净、可破解的代码。安德烈·卡帕蒂着。非常适合从头开始理解 GPT 架构。在 Shakespeare (CPU) 或 OpenWebText (多 GPU) 上进行训练。
