教育 GPT 實現約 300 行。在 OpenWebText 上複製 GPT-2 (124M)。用於學習 Transformer 的干淨、可破解的代碼。安德烈·卡帕蒂著。非常適合從頭開始理解 GPT 架構。在 Shakespeare (CPU) 或 OpenWebText (多 GPU) 上進行訓練。
