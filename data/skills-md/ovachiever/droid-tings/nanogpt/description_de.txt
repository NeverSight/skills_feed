GPT-Implementierung f√ºr den Bildungsbereich in ca. 300 Zeilen. Reproduziert GPT-2 (124M) auf OpenWebText. Sauberer, hackbarer Code zum Erlernen von Transformatoren. Von Andrej Karpathy. Perfekt, um die GPT-Architektur von Grund auf zu verstehen. Trainieren Sie mit Shakespeare (CPU) oder OpenWebText (Multi-GPU).
