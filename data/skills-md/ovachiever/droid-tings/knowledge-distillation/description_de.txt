Komprimieren Sie große Sprachmodelle mithilfe der Wissensdestillation von Lehrer- zu Schülermodellen. Verwenden Sie es, wenn Sie kleinere Modelle mit beibehaltener Leistung bereitstellen, GPT-4-Funktionen auf Open-Source-Modelle übertragen oder Inferenzkosten reduzieren. Behandelt Temperaturskalierung, Soft Targets, Reverse KLD, Logit-Destillation und MiniLLM-Trainingsstrategien.
