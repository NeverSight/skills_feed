Comprima modelos de lenguaje de gran tamaño mediante la destilación de conocimientos de los modelos de profesor a estudiante. Úselo al implementar modelos más pequeños con rendimiento retenido, transferir capacidades GPT-4 a modelos de código abierto o reducir los costos de inferencia. Cubre escalado de temperatura, objetivos suaves, KLD inverso, destilación logit y estrategias de entrenamiento MiniLLM.
