Compressez de grands modèles de langage en utilisant la distillation des connaissances des modèles enseignant vers étudiants. À utiliser lors du déploiement de modèles plus petits avec des performances conservées, du transfert des fonctionnalités GPT-4 vers des modèles open source ou de la réduction des coûts d'inférence. Couvre l'échelle de température, les cibles souples, le KLD inversé, la distillation logit et les stratégies de formation MiniLLM.
