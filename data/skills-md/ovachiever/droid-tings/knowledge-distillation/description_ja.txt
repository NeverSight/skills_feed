教師モデルから生徒モデルへの知識の蒸留を使用して、大規模な言語モデルを圧縮します。パフォーマンスを維持したまま小規模なモデルをデプロイする場合、GPT-4 機能をオープンソース モデルに移行する場合、または推論コストを削減する場合に使用します。温度スケーリング、ソフト ターゲット、逆 KLD、ロジット蒸留、MiniLLM トレーニング戦略をカバーします。
