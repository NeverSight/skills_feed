使用從教師模型到學生模型的知識蒸餾來壓縮大型語言模型。在部署保留性能的較小模型、將 GPT-4 功能轉移到開源模型或降低推理成本時使用。涵蓋溫度縮放、軟目標、反向 KLD、logit 蒸餾和 MiniLLM 訓練策略。
