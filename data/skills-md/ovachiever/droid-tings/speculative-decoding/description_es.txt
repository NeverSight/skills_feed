Acelere la inferencia de LLM mediante decodificación especulativa, cabezas múltiples de Medusa y técnicas de decodificación anticipada. Úselo para optimizar la velocidad de inferencia (aceleración de 1,5 a 3,6 veces), reducir la latencia para aplicaciones en tiempo real o implementar modelos con computación limitada. Cubre modelos preliminares, atención basada en árboles, iteración de Jacobi, generación de tokens paralelos y estrategias de implementación de producción.
