Accélérez l’inférence LLM à l’aide du décodage spéculatif, des têtes multiples Medusa et des techniques de décodage anticipé. À utiliser pour optimiser la vitesse d'inférence (accélération de 1,5 à 3,6 fois), réduire la latence pour les applications en temps réel ou déployer des modèles avec un calcul limité. Couvre les projets de modèles, l'attention basée sur les arbres, l'itération Jacobi, la génération de jetons parallèles et les stratégies de déploiement en production.
