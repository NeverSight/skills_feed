Accelera l'inferenza LLM utilizzando la decodifica speculativa, le teste multiple di Medusa e le tecniche di decodifica lookahead. Da utilizzare per ottimizzare la velocità di inferenza (velocità 1,5-3,6 volte maggiore), ridurre la latenza per applicazioni in tempo reale o distribuire modelli con elaborazione limitata. Copre modelli di bozza, attenzione basata su alberi, iterazione di Jacobi, generazione di token paralleli e strategie di distribuzione della produzione.
