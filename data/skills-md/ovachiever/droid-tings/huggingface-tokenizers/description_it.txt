Tokenizzatori veloci ottimizzati per la ricerca e la produzione. L'implementazione basata su Rust tokenizza 1 GB in <20 secondi. Supporta gli algoritmi BPE, WordPiece e Unigram. Addestra vocabolari personalizzati, tieni traccia degli allineamenti, gestisci il riempimento/troncamento. Si integra perfettamente con i trasformatori. Da utilizzare quando Ã¨ necessaria una tokenizzazione ad alte prestazioni o una formazione personalizzata sui tokenizer.
