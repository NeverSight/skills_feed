Addestra modelli Mixture of Experts (MoE) utilizzando DeepSpeed ​​o HuggingFace. Da utilizzare durante l'addestramento di modelli su larga scala con calcolo limitato (riduzione dei costi 5 volte rispetto a modelli densi), l'implementazione di architetture sparse come Mixtral 8x7B o DeepSeek-V3 o il ridimensionamento della capacità del modello senza aumento proporzionale del calcolo. Copre architetture MoE, meccanismi di routing, bilanciamento del carico, parallelismo esperto e ottimizzazione dell'inferenza.
