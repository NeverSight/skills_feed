使用 DeepSpeed 或 HuggingFace 训练混合专家 (MoE) 模型。在训练计算量有限的大型模型（与密集模型相比成本降低 5 倍）、实施 Mixtral 8x7B 或 DeepSeek-V3 等稀疏架构或在不按比例增加计算量的情况下扩展模型容量时使用。涵盖 MoE 架构、路由机制、负载均衡、专家并行和推理优化。
