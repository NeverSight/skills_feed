Entrene modelos de mezcla de expertos (MoE) utilizando DeepSpeed ​​o HuggingFace. Úselo al entrenar modelos a gran escala con computación limitada (reducción de costos 5 veces frente a modelos densos), implementar arquitecturas dispersas como Mixtral 8x7B o DeepSeek-V3, o escalar la capacidad del modelo sin un aumento de computación proporcional. Cubre arquitecturas MoE, mecanismos de enrutamiento, equilibrio de carga, paralelismo experto y optimización de inferencia.
