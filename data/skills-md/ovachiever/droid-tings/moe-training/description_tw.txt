使用 DeepSpeed 或 HuggingFace 訓練混合專家 (MoE) 模型。在訓練計算量有限的大型模型（與密集模型相比成本降低 5 倍）、實施 Mixtral 8x7B 或 DeepSeek-V3 等稀疏架構或在不按比例增加計算量的情況下擴展模型容量時使用。涵蓋 MoE 架構、路由機制、負載均衡、專家並行和推理優化。
