DeepSpeed または HuggingFace を使用して、Mixture of Experts (MoE) モデルをトレーニングします。限られたコンピューティング (高密度モデルと比較して 5 倍のコスト削減) で大規模モデルをトレーニングする場合、Mixtral 8x7B や DeepSeek-V3 などのスパース アーキテクチャを実装する場合、または比例的にコンピューティングを増加させることなくモデル容量をスケーリングする場合に使用します。 MoE アーキテクチャ、ルーティング メカニズム、負荷分散、エキスパート並列処理、推論の最適化について説明します。
