Riduci le dimensioni del LLM e accelera l'inferenza utilizzando tecniche di potatura come Wanda e SparseGPT. Da utilizzare per comprimere modelli senza riqualificazione, ottenendo una scarsità del 50% con una perdita di precisione minima o consentendo un'inferenza più rapida sugli acceleratori hardware. Copre la potatura non strutturata, la potatura strutturata, la scarsità N:M, la potatura della magnitudo e i metodi one-shot.
