Reduzca el tamaño de LLM y acelere la inferencia utilizando técnicas de poda como Wanda y SparseGPT. Úselo al comprimir modelos sin volver a entrenar, lograr un 50 % de escasez con una pérdida de precisión mínima o permitir una inferencia más rápida en aceleradores de hardware. Cubre la poda no estructurada, la poda estructurada, la escasez N:M, la poda de magnitud y los métodos de una sola vez.
