Optimiert die LLM-Inferenz mit NVIDIA TensorRT für maximalen Durchsatz und niedrigste Latenz. Verwenden Sie es für die Produktionsbereitstellung auf NVIDIA-GPUs (A100/H100), wenn Sie eine 10–100-mal schnellere Inferenz als PyTorch benötigen, oder für die Bereitstellung von Modellen mit Quantisierung (FP8/INT4), In-Flight-Batching und Multi-GPU-Skalierung.
