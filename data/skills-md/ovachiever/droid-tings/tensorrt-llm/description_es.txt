Optimiza la inferencia LLM con NVIDIA TensorRT para obtener el máximo rendimiento y la menor latencia. Úselo para la implementación de producción en GPU NVIDIA (A100/H100), cuando necesite una inferencia entre 10 y 100 veces más rápida que PyTorch, o para ofrecer modelos con cuantificación (FP8/INT4), procesamiento por lotes en vuelo y escalado de múltiples GPU.
