Generazione e servizio strutturati rapidi per LLM con memorizzazione nella cache del prefisso RadixAttention. Utilizzalo per output JSON/regex, decodifica vincolata, flussi di lavoro agenti con chiamate a strumenti o quando hai bisogno di un'inferenza 5 volte pi√π veloce rispetto a vLLM con condivisione del prefisso. Alimenta oltre 300.000 GPU presso xAI, AMD, NVIDIA e LinkedIn.
