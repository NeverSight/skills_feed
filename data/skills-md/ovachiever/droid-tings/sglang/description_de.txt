Schnelle strukturierte Generierung und Bereitstellung für LLMs mit RadixAttention-Präfix-Caching. Verwenden Sie es für JSON-/Regex-Ausgaben, eingeschränkte Dekodierung, Agenten-Workflows mit Tool-Aufrufen oder wenn Sie eine 5-mal schnellere Inferenz als vLLM mit Präfixfreigabe benötigen. Betreibt über 300.000 GPUs bei xAI, AMD, NVIDIA und LinkedIn.
