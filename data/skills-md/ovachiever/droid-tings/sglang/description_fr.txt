Génération structurée rapide et service pour les LLM avec la mise en cache du préfixe RadixAttention. À utiliser pour les sorties JSON/regex, le décodage contraint, les flux de travail agents avec appels d'outils ou lorsque vous avez besoin d'une inférence 5 fois plus rapide que vLLM avec partage de préfixe. Alimente plus de 300 000 GPU chez xAI, AMD, NVIDIA et LinkedIn.
