精度損失を最小限に抑えた LLM のトレーニング後の 4 ビット量子化。パープレキシティの低下が 2% 未満でメモリを 4 倍削減する必要がある場合、または FP16 と比較してより高速な推論 (3 ～ 4 倍のスピードアップ) が必要な場合に、コンシューマー GPU に大規模なモデル (70B、405B) をデプロイする場合に使用します。 QLoRA 微調整のためにトランスフォーマーおよび PEFT と統合されています。
