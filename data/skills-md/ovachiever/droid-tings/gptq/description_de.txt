4-Bit-Quantisierung nach dem Training für LLMs mit minimalem Genauigkeitsverlust. Verwenden Sie diese Option für die Bereitstellung großer Modelle (70B, 405B) auf Consumer-GPUs, wenn Sie eine 4-fache Speicherreduzierung mit einer Perplexitätsverschlechterung von <2 % benötigen, oder für eine schnellere Inferenz (3-4-fache Beschleunigung) im Vergleich zu FP16. Lässt sich mit Transformatoren und PEFT zur QLoRA-Feinabstimmung integrieren.
