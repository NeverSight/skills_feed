Quantizzazione a 4 bit post-addestramento per LLM con perdita di precisione minima. Da utilizzare per la distribuzione di modelli di grandi dimensioni (70B, 405B) su GPU consumer, quando è necessaria una riduzione della memoria di 4 volte con un degrado di perplessità <2% o per un'inferenza più rapida (aumento di velocità di 3-4 volte) rispetto a FP16. Si integra con trasformatori e PEFT per la regolazione fine QLoRA.
