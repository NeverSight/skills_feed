Cuantización de 4 bits posterior al entrenamiento para LLM con una pérdida de precisión mínima. Úselo para implementar modelos grandes (70B, 405B) en GPU de consumo, cuando necesite una reducción de memoria 4 veces con una degradación de perplejidad <2 %, o para una inferencia más rápida (aceleración de 3-4 veces) en comparación con FP16. Se integra con transformadores y PEFT para ajuste fino de QLoRA.
