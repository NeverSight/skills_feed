Quantification 4 bits post-formation pour les LLM avec une perte de précision minimale. À utiliser pour déployer de grands modèles (70B, 405B) sur des GPU grand public, lorsque vous avez besoin d'une réduction de mémoire 4× avec une dégradation de perplexité <2 %, ou pour une inférence plus rapide (accélération 3-4×) par rapport au FP16. S'intègre aux transformateurs et au PEFT pour un réglage précis de QLoRA.
