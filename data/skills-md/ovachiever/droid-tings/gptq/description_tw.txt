LLM 的訓練後 4 位量化，精度損失最小。當您需要 4 倍內存減少且困惑度降低 <2% 時，或者需要比 FP16 更快的推理（3-4 倍加速）時，可用於在消費類 GPU 上部署大型模型（70B、405B）。與變壓器和 PEFT 集成以進行 QLoRA 微調。
