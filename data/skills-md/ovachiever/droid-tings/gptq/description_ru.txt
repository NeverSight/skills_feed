4-битное квантование после обучения для LLM с минимальной потерей точности. Используйте для развертывания больших моделей (70B, 405B) на потребительских графических процессорах, когда вам нужно сокращение памяти в 4 раза с ухудшением сложности менее 2% или для более быстрого вывода (ускорение в 3–4 раза) по сравнению с FP16. Интегрируется с трансформаторами и PEFT для точной настройки QLoRA.
