LLM 的训练后 4 位量化，精度损失最小。当您需要 4 倍内存减少且困惑度降低 <2% 时，或者需要比 FP16 更快的推理（3-4 倍加速）时，可用于在消费类 GPU 上部署大型模型（70B、405B）。与变压器和 PEFT 集成以进行 QLoRA 微调。
