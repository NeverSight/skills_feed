低ランク適応 (LoRA) によるパラメータ効率の高い微調整。限られた GPU メモリで大規模な言語モデルを微調整する場合、タスク固有のアダプターを作成する場合、または単一のベースから複数の特殊なモデルをトレーニングする必要がある場合に使用します。
