언어 모델 정렬을 위한 인간 피드백(RLHF)의 강화 학습 이해 선호도 데이터, 보상 모델링, 정책 최적화 또는 DPO와 같은 직접 정렬 알고리즘에 대해 학습할 때 사용합니다.
