Comprendere l'apprendimento per rinforzo dal feedback umano (RLHF) per allineare i modelli linguistici. Da utilizzare per conoscere i dati sulle preferenze, la modellazione dei premi, l'ottimizzazione delle politiche o gli algoritmi di allineamento diretto come DPO.
