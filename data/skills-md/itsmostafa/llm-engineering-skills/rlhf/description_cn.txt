了解人类反馈中的强化学习 (RLHF)，以调整语言模型。在学习偏好数据、奖励建模、策略优化或直接对齐算法（如 DPO）时使用。
