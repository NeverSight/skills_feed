Comprendre l'apprentissage par renforcement à partir de la rétroaction humaine (RLHF) pour aligner les modèles de langage. À utiliser pour découvrir les données de préférences, la modélisation des récompenses, l'optimisation des politiques ou les algorithmes d'alignement direct tels que DPO.
