言語モデルを調整するためのヒューマン フィードバックからの強化学習 (RLHF) について理解します。嗜好データ、報酬モデリング、ポリシーの最適化、または DPO などの直接調整アルゴリズムについて学習するときに使用します。
