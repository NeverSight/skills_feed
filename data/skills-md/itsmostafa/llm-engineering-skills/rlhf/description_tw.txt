了解人類回饋中的強化學習 (RLHF)，以調整語言模型。在學習偏好資料、獎勵建模、策略最佳化或直接對齊演算法（如 DPO）時使用。
