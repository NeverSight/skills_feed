LLM-as-Judge-Methodik zum Vergleich von Code-Implementierungen über Repositorys hinweg. Bewertet Implementierungen anhand gewichteter Rubriken hinsichtlich Funktionalität, Sicherheit, Testqualität, Overengineering und totem Code. Wird vom Befehl /beagle:llm-judge verwendet.
