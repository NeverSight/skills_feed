---
name: module2-tokens-context
description: tokenization과 context window를 중심으로 긴 입력 처리 한계와 실무 대응 방법(분할, 요약, 우선순위화)을 학습시키는 모듈.
---

# Module 2: Tokens and Context

핵심 질문: 긴 문서를 넣으면 왜 품질이 흔들리는가?

## 학습 목표

- token과 문자/단어의 차이를 이해한다.
- context window의 의미와 한계를 설명할 수 있다.
- 긴 입력 처리 시 분할/요약/우선순위화 전략을 적용할 수 있다.

## 교차 복습 연결

- Module 1에서 배운 "토큰"과 "확률적 생성"을 이 모듈의 출발점으로 자연스럽게 재등장시킨다.

## 진행 구조 (4단계, 목표 10~15분)

이 모듈은 반드시 아래 4단계를 순서대로 진행한다. 단계를 건너뛰지 않는다.

### Phase 1: 문제 도입 (2~3분, 최소 1회 학습자 응답 필요)

- 학습자의 직무에서 "긴 문서를 AI에 넣었는데 결과가 이상했던" 경험을 끌어낸다.
- 예: 긴 보고서, PRD, 로그 파일 등을 AI에 붙여넣기한 경험
- Module 1에서 배운 "토큰" 개념을 자연스럽게 상기시킨다: "Module 1에서 LLM이 토큰 단위로 처리한다고 했는데, 그 토큰에 한계가 있다면?"
- 정의 설명 없이 경험/현상에서 출발한다.
- 학습자 응답을 받은 후에야 Phase 2로 진행한다.

### Phase 2: 핵심 개념 탐구 (5~7분, 최소 3회 학습자 응답 필요)

아래 개념을 순서대로 다룬다. **한 응답에서 2개 이상의 새로운 개념을 동시에 설명하지 않는다.**

1. **tokenization 심화** — 한국어/영어 토큰화 차이, 같은 문장도 토큰 수가 다른 이유
2. **context window** — 입력+출력 합산 제한, 모델별 차이
3. **prompt budget** — context window 안에서 시스템/사용자/출력 공간 배분

각 개념마다:
- ASCII 표나 흐름도로 토큰 소모를 시각화한다.
- 실제 긴 문서 예시(보고서, PRD, 로그)로 설명한다.
- 설명 후 반드시 학습자에게 확인 질문을 던지고 **응답을 기다린다**.
- 학습자가 응답한 후에야 다음 개념으로 넘어간다.

### Phase 3: 연결 및 적용 (2~3분, 최소 1회 학습자 응답 필요)

- **chunking의 필요성**을 도입한다.
- "무엇을 남기고 무엇을 버릴지"를 학습자가 **직접 판단**하게 한다.
- 학습자의 실제 업무 문서 유형을 기준으로 분할/요약/우선순위화 전략을 토론한다.
- 학습자의 응답을 받은 후에야 Phase 4로 진행한다.

### Phase 4: 이해 확인 (2~3분, 최소 1회 학습자 응답 필요)

완료 기준 질문을 제시한다. 학습자가 **자신의 업무 문서 1개를 기준으로** 답해야 한다.

1. 입력 분할 기준
2. 우선 포함할 정보 3가지
3. 제외/요약할 정보 1가지 이상

- 학습자의 답이 부족하면 Phase 2~3의 관련 부분으로 돌아가 보충한다.
- 충분하면 모듈 완료를 선언하고 다음 모듈을 안내한다.

## 페이스 규칙 (필수)

- **한 응답에서 2개 이상의 새로운 개념을 동시에 설명하지 않는다.**
- **학습자가 응답하지 않은 상태에서 다음 개념으로 넘어가지 않는다.**
- **최소 교환 횟수: AI 6회 응답 + 학습자 6회 응답 = 12턴 이상이어야 모듈 완료 가능.**
- 12턴 미만에서 완료 기준을 충족하더라도, 추가 연결 질문이나 적용 시나리오로 깊이를 확보한다.
- 선택지가 필요한 분기점에서는 `AskUserQuestion` 도구를 사용한다.

## 개념 체계 (기본 → 심화)

### 기본 (반드시 다룸)

| 개념 | 핵심 한 줄 | Phase |
|---|---|---|
| tokenization | 텍스트를 토큰으로 쪼개는 과정 — 언어별, 모델별로 방식이 다름 | 2 |
| context window | 모델이 한 번에 처리할 수 있는 토큰의 총량 (입력+출력 합산) | 2 |
| prompt budget | context window 안에서 시스템/사용자/출력 공간을 어떻게 배분할지 | 2 |
| chunking | 긴 문서를 의미 단위로 분할하는 전략 | 3 |

### 심화 (학습자가 관심을 보이거나, 경험 수준이 높을 때 확장)

| 개념 | 핵심 한 줄 | 언제 다루나 |
|---|---|---|
| BPE (Byte Pair Encoding) | 대부분의 LLM이 사용하는 토큰화 알고리즘의 원리 | tokenization에서 "왜 한국어가 토큰을 더 많이 쓰나?" 질문 시 |
| 모델별 context window 비교 | GPT-4(128K), Claude(200K), Gemini(1M~2M) 등 실제 수치 비교 | context window 크기에 관심을 보일 때 |
| lost-in-the-middle 현상 | 긴 context의 가운데 정보를 모델이 놓치는 문제 | context window를 이해한 후 |
| 요약 전략 (map-reduce, refine) | 긴 문서를 처리하는 체계적 요약 패턴 | chunking 이후 "어떻게 요약하나?" 질문 시 |
| 비용 최적화 | 토큰 수를 줄이는 프롬프트 최적화, 캐싱, 모델 선택 전략 | prompt budget에서 비용에 관심을 보일 때 |
| streaming 출력 | 토큰 단위 실시간 출력의 원리와 UX 영향 | "왜 글자가 하나씩 나오나?" 질문 시 |

### 심화 개념 진행 규칙

- 기본 개념이 모두 완료된 후에만 심화로 확장한다.
- 학습자가 관심을 보이거나 AI 경험이 높을 때 자연스럽게 도입한다.
- 심화 개념은 Phase 3 또는 Phase 4 이후 보너스로 다루되, 모듈 완료 기준에는 포함하지 않는다.
- 학습자가 원하지 않으면 넘어간다.

## 완료 기준

학습자가 자신의 업무 문서 1개를 기준으로 아래를 제시하면 완료한다.

1. 입력 분할 기준
2. 우선 포함할 정보 3가지
3. 제외/요약할 정보 1가지 이상

## 다음 연결

모듈 완료 시 다음 선택지를 `AskUserQuestion` 도구로 제시한다.

- 외부 지식을 검색해 붙이는 방식은 `module3-rag-embedding`
