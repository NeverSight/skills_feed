Sicherheitsmuster für LLM-Integrationen, einschließlich sofortiger Injektionsabwehr und Halluzinationsprävention. Verwenden Sie diese Option, wenn Sie eine Kontexttrennung implementieren, LLM-Ausgaben validieren oder sich vor Prompt-Injection-Angriffen schützen.
