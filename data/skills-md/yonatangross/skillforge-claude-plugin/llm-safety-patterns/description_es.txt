Patrones de seguridad para integraciones de LLM que incluyen defensa contra inyecciones rápidas y prevención de alucinaciones. Úselo al implementar la separación de contexto, validar los resultados de LLM o proteger contra ataques de inyección rápida.
