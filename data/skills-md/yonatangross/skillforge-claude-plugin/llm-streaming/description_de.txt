LLM-Streaming-Antwortmuster. Verwenden Sie diese Option, wenn Sie Echtzeit-Token-Streaming, vom Server gesendete Ereignisse f√ºr KI-Antworten oder Streaming mit Tool-Aufrufen implementieren.
