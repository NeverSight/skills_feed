Inférence LLM locale avec Ollama. À utiliser lors de la configuration de modèles locaux pour le développement, les pipelines CI ou la réduction des coûts. Couvre la sélection du modèle, l'intégration de LangChain et le réglage des performances.
