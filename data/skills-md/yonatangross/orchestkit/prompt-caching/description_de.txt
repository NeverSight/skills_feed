Anbieternatives Prompt-Caching für Claude und OpenAI. Verwenden Sie diese Option, wenn Sie die LLM-Kosten mit Cache-Haltepunkten optimieren, Systemaufforderungen zwischenspeichern oder die Token-Kosten für wiederholte Präfixe reduzieren.
