Inferencia LLM de alto rendimiento con vLLM, cuantificación (AWQ, GPTQ, FP8), decodificación especulativa e implementación perimetral. Utilícelo para optimizar la latencia, el rendimiento o la memoria de inferencia.
