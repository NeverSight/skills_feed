Inferenza LLM ad alte prestazioni con vLLM, quantizzazione (AWQ, GPTQ, FP8), decodifica speculativa e distribuzione edge. Da utilizzare per ottimizzare la latenza di inferenza, la velocit√† effettiva o la memoria.
