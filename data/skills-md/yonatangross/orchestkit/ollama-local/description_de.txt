Lokale LLM-Inferenz mit Ollama. Verwendung beim Einrichten lokaler Modelle f√ºr Entwicklung, CI-Pipelines oder Kostenreduzierung. Behandelt Modellauswahl, LangChain-Integration und Leistungsoptimierung.
