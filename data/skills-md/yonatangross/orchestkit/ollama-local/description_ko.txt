Ollama를 사용한 로컬 LLM 추론. 개발, CI 파이프라인 또는 비용 절감을 위해 로컬 모델을 설정할 때 사용합니다. 모델 선택, LangChain 통합 및 성능 튜닝을 다룹니다.
