Inferenza LLM locale con Ollama. Da utilizzare durante l'impostazione di modelli locali per lo sviluppo, pipeline CI o riduzione dei costi. Copre la selezione del modello, l'integrazione di LangChain e l'ottimizzazione delle prestazioni.
