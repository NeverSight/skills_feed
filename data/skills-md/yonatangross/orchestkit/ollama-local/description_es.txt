Inferencia local del LLM con Ollama. Úselo al configurar modelos locales para desarrollo, canalizaciones de CI o reducción de costos. Cubre la selección de modelos, la integración de LangChain y el ajuste del rendimiento.
