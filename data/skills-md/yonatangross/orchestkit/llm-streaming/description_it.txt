Modelli di risposta allo streaming LLM. Da utilizzare durante l'implementazione dello streaming di token in tempo reale, degli eventi inviati dal server per le risposte AI o dello streaming con chiamate agli strumenti.
