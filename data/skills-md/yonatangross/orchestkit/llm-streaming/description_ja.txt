LLM ストリーミング応答パターン。リアルタイムのトークン ストリーミング、AI 応答用のサーバー送信イベント、またはツール呼び出しによるストリーミングを実装する場合に使用します。
