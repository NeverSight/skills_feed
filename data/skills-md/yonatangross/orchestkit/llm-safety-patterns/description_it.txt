Modelli di sicurezza per le integrazioni LLM, tra cui la difesa immediata dall'iniezione e la prevenzione delle allucinazioni. Da utilizzare quando si implementa la separazione del contesto, si convalidano gli output LLM o si protegge da attacchi di iniezione tempestiva.
