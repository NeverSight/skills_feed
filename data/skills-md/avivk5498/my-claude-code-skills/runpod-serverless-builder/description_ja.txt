最適化されたコールド スタート時間を使用して、本番環境に対応した RunPod サーバーレス エンドポイントを構築します。 (1) vLLM ベースの LLM 推論、(2) ComfyUI 画像/ビデオ生成、または (3) カスタム Python 推論のために RunPod サーバーレス ワーカーを作成または変更するときに使用します。ベイクされたモデル (最速のコールド スタート) と動的読み込み (共有モデル) の両方をサポートします。 Dockerfile、ワーカー ハンドラー、起動スクリプト、コールド スタートの待ち時間を最小限に抑えるために最適化された構成を含む完全なプロジェクトを生成します。
