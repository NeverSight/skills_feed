Erstellen Sie produktionsbereite serverlose RunPod-Endpunkte mit optimierten Kaltstartzeiten. Wird zum Erstellen oder Ändern serverloser RunPod-Worker für (1) vLLM-basierte LLM-Inferenz, (2) ComfyUI-Bild-/Videogenerierung oder (3) benutzerdefinierte Python-Inferenz verwendet. Unterstützt sowohl gebackene Modelle (schnellste Kaltstarts) als auch dynamisches Laden (gemeinsam genutzte Modelle). Erzeugt komplette Projekte, einschließlich Docker-Dateien, Worker-Handler, Startskripts und Konfiguration, optimiert für minimale Kaltstart-Latenz.
