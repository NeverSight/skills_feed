Estimez l’utilisation de la mémoire GPU pour les modèles MoE (mélange d’experts) et denses basés sur Megatron. À utiliser lorsque les utilisateurs doivent (1) estimer la mémoire à partir des configurations du modèle HuggingFace (DeepSeek-V3, Qwen, etc.), (2) planifier l'allocation des ressources GPU pour la formation, (3) comparer différentes stratégies de parallélisme (TP/PP/EP/CP), (4) déterminer si un modèle tient dans la mémoire GPU disponible ou (5) optimiser les configurations de formation pour l'efficacité de la mémoire.
