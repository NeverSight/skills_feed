Megatron ベースの MoE (専門家の混合) および高密度モデルの GPU メモリ使用量を見積もります。ユーザーが (1) HuggingFace モデル構成 (DeepSeek-V3、Qwen など) からメモリを推定する、(2) トレーニング用の GPU リソース割り当てを計画する、(3) さまざまな並列処理戦略 (TP/PP/EP/CP) を比較する、(4) モデルが利用可能な GPU メモリに適合するかどうかを判断する、または (5) メモリ効率のためにトレーニング構成を最適化する必要がある場合に使用します。
