Schätzen Sie die GPU-Speichernutzung für Megatron-basierte MoE-Modelle (Mixture of Experts) und dichte Modelle. Wird verwendet, wenn Benutzer (1) den Speicher aus HuggingFace-Modellkonfigurationen (DeepSeek-V3, Qwen usw.) schätzen müssen, (2) die GPU-Ressourcenzuweisung für das Training planen, (3) verschiedene Parallelitätsstrategien (TP/PP/EP/CP) vergleichen, (4) bestimmen müssen, ob ein Modell in den verfügbaren GPU-Speicher passt, oder (5) Trainingskonfigurationen für die Speichereffizienz optimieren müssen.
