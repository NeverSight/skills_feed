Stima dell'utilizzo della memoria della GPU per MoE (Mixture of Experts) basato su Megatron e modelli densi. Da utilizzare quando gli utenti devono (1) stimare la memoria dalle configurazioni del modello HuggingFace (DeepSeek-V3, Qwen e cos√¨ via), (2) pianificare l'allocazione delle risorse GPU per l'addestramento, (3) confrontare diverse strategie di parallelismo (TP/PP/EP/CP), (4) determinare se un modello si adatta alla memoria GPU disponibile o (5) ottimizzare le configurazioni di addestramento per l'efficienza della memoria.
