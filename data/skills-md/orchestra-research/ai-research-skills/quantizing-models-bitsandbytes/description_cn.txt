将 LLM 量化为 8 位或 4 位，可减少 50-75% 的内存，同时将精度损失降至最低。当 GPU 内存有限、需要适应更大的模型或想要更快的推理时使用。支持 INT8、NF4、FP4 格式、QLoRA 训练和 8 位优化器。与 HuggingFace 变形金刚一起使用。
