Optimise l'attention du transformateur avec Flash Attention pour une accélération de 2 à 4x et une réduction de mémoire de 10 à 20x. À utiliser lors de l'entraînement/de l'exécution de transformateurs avec de longues séquences (> 512 jetons), lorsque vous rencontrez des problèmes de mémoire GPU avec attention ou lorsque vous avez besoin d'une inférence plus rapide. Prend en charge le SDPA natif de PyTorch, la bibliothèque flash-attn, le H100 FP8 et l'attention des fenêtres coulissantes.
