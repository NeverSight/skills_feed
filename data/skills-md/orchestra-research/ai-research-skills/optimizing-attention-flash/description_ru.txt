Оптимизирует внимание трансформера с помощью Flash Attention для ускорения в 2–4 раза и сокращения памяти в 10–20 раз. Используйте при обучении/запуске преобразователей с длинными последовательностями (>512 токенов), при возникновении проблем с памятью графического процессора или при необходимости более быстрого вывода. Поддерживает встроенный SDPA PyTorch, библиотеку flash-attn, H100 FP8 и внимание к скользящему окну.
