フラッシュ アテンションによりトランス アテンションを最適化し、2 ～ 4 倍のスピードアップと 10 ～ 20 倍のメモリ削減を実現します。長いシーケンス (>512 トークン) を使用してトランスフォーマーをトレーニング/実行する場合、GPU メモリの問題に注意が必要な場合、またはより高速な推論が必要な場合に使用します。 PyTorch ネイティブ SDPA、フラッシュ attn ライブラリ、H100 FP8、およびスライディング ウィンドウ アテンションをサポートします。
