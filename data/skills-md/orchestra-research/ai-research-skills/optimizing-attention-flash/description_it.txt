Ottimizza l'attenzione del trasformatore con Flash Attention per un aumento della velocità di 2-4 volte e una riduzione della memoria di 10-20 volte. Da utilizzare quando si addestrano/eseguono trasformatori con sequenze lunghe (>512 token), si riscontrano problemi di memoria della GPU con attenzione o è necessaria un'inferenza più rapida. Supporta SDPA nativo PyTorch, libreria flash-attn, H100 FP8 e attenzione tramite finestra scorrevole.
