يعمل على تحسين انتباه المحولات من خلال Flash Attention لتسريع 2-4x وتقليل الذاكرة 10-20x. يُستخدم عند تدريب/تشغيل المحولات بتسلسلات طويلة (> 512 رمزًا)، أو عند مواجهة مشكلات في ذاكرة وحدة معالجة الرسومات (GPU) مع الانتباه، أو عند الحاجة إلى استنتاج أسرع. يدعم PyTorch الأصلي SDPA ومكتبة flash-attn وH100 FP8 وانتبه النافذة المنزلقة.
