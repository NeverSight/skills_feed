Serve LLM con un throughput elevato utilizzando PagedAttention di vLLM e il batch continuo. Da utilizzare quando si distribuiscono API LLM di produzione, si ottimizza la latenza/velocit√† effettiva di inferenza o si servono modelli con memoria GPU limitata. Supporta endpoint compatibili con OpenAI, quantizzazione (GPTQ/AWQ/FP8) e parallelismo tensore.
