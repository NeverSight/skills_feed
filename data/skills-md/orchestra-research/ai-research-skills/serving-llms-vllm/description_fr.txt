Sert des LLM avec un débit élevé à l'aide de PagedAttention et du traitement par lots continu de vLLM. À utiliser lors du déploiement d'API LLM de production, de l'optimisation de la latence/débit d'inférence ou de la diffusion de modèles avec une mémoire GPU limitée. Prend en charge les points de terminaison compatibles OpenAI, la quantification (GPTQ/AWQ/FP8) et le parallélisme tensoriel.
