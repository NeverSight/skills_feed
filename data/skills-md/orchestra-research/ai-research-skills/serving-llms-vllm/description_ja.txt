vLLM の PagedAttendance と連続バッチ処理を使用して、高スループットで LLM を提供します。実稼働 LLM API をデプロイする場合、推論レイテンシー/スループットを最適化する場合、または限られた GPU メモリでモデルを提供する場合に使用します。 OpenAI 互換エンドポイント、量子化 (GPTQ/AWQ/FP8)、テンソル並列処理をサポートします。
