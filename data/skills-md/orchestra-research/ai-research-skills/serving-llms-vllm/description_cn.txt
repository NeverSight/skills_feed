使用 vLLM 的 PagedAttention 和连续批处理为法学硕士提供高吞吐量服务。在部署生产 LLM API、优化推理延迟/吞吐量或为 GPU 内存有限的模型提供服务时使用。支持 OpenAI 兼容端点、量化 (GPTQ/AWQ/FP8) 和张量并行性。
