vLLM의 PagedAttention 및 연속 일괄 처리를 사용하여 높은 처리량으로 LLM을 제공합니다. 프로덕션 LLM API를 배포하거나, 추론 지연 시간/처리량을 최적화하거나, GPU 메모리가 제한된 모델을 제공할 때 사용하세요. OpenAI 호환 엔드포인트, 양자화(GPTQ/AWQ/FP8) 및 텐서 병렬 처리를 지원합니다.
