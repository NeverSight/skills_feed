使用 vLLM 的 PagedAttention 和連續批次為法學碩士提供高吞吐量服務。在部署生產 LLM API、最佳化推理延遲/吞吐量或為 GPU 記憶體有限的模型提供服務時使用。支援 OpenAI 相容端點、量化 (GPTQ/AWQ/FP8) 和張量並行性。
