使用从教师模型到学生模型的知识蒸馏来压缩大型语言模型。在部署保留性能的较小模型、将 GPT-4 功能转移到开源模型或降低推理成本时使用。涵盖温度缩放、软目标、反向 KLD、logit 蒸馏和 MiniLLM 训练策略。
