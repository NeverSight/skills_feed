アクティベーションを意識した重み量子化による 4 ビット LLM 圧縮により、3 倍のスピードアップと最小限の精度損失が実現します。限られた GPU メモリに大規模なモデル (7B ～ 70B) をデプロイする場合、精度を維持しながら GPTQ よりも高速な推論が必要な場合、または命令調整されたマルチモーダル モデルに使用します。 MLSys 2024 Best Paper Award受賞。
