Aktivierungsbewusste Gewichtsquantisierung für 4-Bit-LLM-Komprimierung mit 3-facher Beschleunigung und minimalem Genauigkeitsverlust. Verwenden Sie diese Option, wenn Sie große Modelle (7B-70B) auf begrenztem GPU-Speicher bereitstellen, wenn Sie eine schnellere Inferenz als GPTQ mit besserer Genauigkeitserhaltung benötigen, oder für anweisungsabgestimmte und multimodale Modelle. Gewinner des MLSys 2024 Best Paper Award.
