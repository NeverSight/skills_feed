Fornisce preformazione LLM distribuito nativo di PyTorch utilizzando torchtitan con parallelismo 4D (FSDP2, TP, PP, CP). Da utilizzare durante il pre-addestramento di Llama 3.1, DeepSeek V3 o modelli personalizzati su scala da 8 a 512+ GPU con Float8, torch.compile e checkpoint distribuito.
