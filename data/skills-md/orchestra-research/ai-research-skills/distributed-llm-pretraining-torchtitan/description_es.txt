Proporciona capacitación previa de LLM distribuida nativa de PyTorch utilizando torchtitan con paralelismo 4D (FSDP2, TP, PP, CP). Úselo al realizar un entrenamiento previo de Llama 3.1, DeepSeek V3 o modelos personalizados a escala de 8 a más de 512 GPU con Float8, torch.compile y puntos de control distribuidos.
