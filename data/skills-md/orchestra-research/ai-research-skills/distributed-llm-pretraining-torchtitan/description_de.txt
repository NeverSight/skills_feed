Bietet PyTorch-natives verteiltes LLM-Vortraining mit Torchtitan mit 4D-Parallelität (FSDP2, TP, PP, CP). Zur Verwendung beim Vortraining von Llama 3.1, DeepSeek V3 oder benutzerdefinierten Modellen im Maßstab von 8 bis 512+ GPUs mit Float8, Torch.compile und verteiltem Checkpointing.
