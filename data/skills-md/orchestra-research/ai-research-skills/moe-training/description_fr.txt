Entraînez des modèles Mixture of Experts (MoE) à l’aide de DeepSpeed ​​ou de HuggingFace. À utiliser lors de la formation de modèles à grande échelle avec un calcul limité (réduction des coûts de 5 fois par rapport aux modèles denses), lors de la mise en œuvre d'architectures clairsemées telles que Mixtral 8x7B ou DeepSeek-V3, ou lors de la mise à l'échelle de la capacité du modèle sans augmentation proportionnelle du calcul. Couvre les architectures MoE, les mécanismes de routage, l'équilibrage de charge, le parallélisme expert et l'optimisation des inférences.
