تدريب نماذج مزيج من الخبراء (MoE) باستخدام DeepSpeed ​​أو HuggingFace. يُستخدم عند تدريب نماذج واسعة النطاق ذات حوسبة محدودة (تخفيض التكلفة بمقدار 5 × مقابل النماذج الكثيفة)، أو تنفيذ بنيات متفرقة مثل Mixtral 8x7B أو DeepSeek-V3، أو توسيع سعة النموذج دون زيادة حوسبة متناسبة. يغطي بنيات وزارة البيئة، وآليات التوجيه، وموازنة التحميل، والتوازي الخبراء، وتحسين الاستدلال.
