API di formazione distribuita pi√π semplice. 4 righe per aggiungere supporto distribuito a qualsiasi script PyTorch. API unificata per DeepSpeed/FSDP/Megatron/DDP. Posizionamento automatico del dispositivo, precisione mista (FP16/BF16/FP8). Configurazione interattiva, comando di avvio singolo. Standard dell'ecosistema HuggingFace.
