Beschleunigen Sie die LLM-Inferenz durch spekulative Dekodierung, Medusa-Mehrfachköpfe und Lookahead-Dekodierungstechniken. Verwenden Sie diese Option, wenn Sie die Inferenzgeschwindigkeit optimieren (1,5- bis 3,6-fache Beschleunigung), die Latenz für Echtzeitanwendungen reduzieren oder Modelle mit begrenzter Rechenleistung bereitstellen. Behandelt Entwurfsmodelle, baumbasierte Aufmerksamkeit, Jacobi-Iteration, parallele Token-Generierung und Produktionsbereitstellungsstrategien.
