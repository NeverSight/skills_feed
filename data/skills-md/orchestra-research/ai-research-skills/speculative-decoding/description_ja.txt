投機的デコード、Medusa マルチヘッド、先読みデコード技術を使用して LLM 推論を高速化します。推論速度の最適化 (1.5 ～ 3.6 倍の高速化)、リアルタイム アプリケーションの遅延の削減、または限られたコンピューティングでモデルを展開する場合に使用します。ドラフト モデル、ツリーベースのアテンション、Jacobi 反復、並列トークン生成、および運用展開戦略について説明します。
