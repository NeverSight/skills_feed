Generación estructurada rápida y servicio para LLM con almacenamiento en caché de prefijo RadixAttention. Úselo para salidas JSON/regex, decodificación restringida, flujos de trabajo agentes con llamadas a herramientas o cuando necesite una inferencia 5 veces más rápida que vLLM con uso compartido de prefijos. Alimenta más de 300 000 GPU en xAI, AMD, NVIDIA y LinkedIn.
