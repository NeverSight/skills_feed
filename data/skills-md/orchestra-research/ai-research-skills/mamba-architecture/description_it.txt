Modello spazio-stato con complessità O(n) vs O(n²) di Transformers. Inferenza 5 volte più veloce, sequenze da milioni di token, nessuna cache KV. SSM selettivo con design compatibile con l'hardware. Mamba-1 (d_state=16) e Mamba-2 (d_state=128, multitesta). Modelli 130M-2.8B su HuggingFace.
