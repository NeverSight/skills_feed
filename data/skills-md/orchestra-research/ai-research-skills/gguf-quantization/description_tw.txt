GGUF 格式和 llama.cpp 量化可實現高效的 CPU/GPU 推理。在消費性硬體、Apple Silicon 上部署模型時，或需要從 2-8 位元進行靈活量化且無需 GPU 時使用。
