GGUF 格式和 llama.cpp 量化可实现高效的 CPU/GPU 推理。在消费类硬件、Apple Silicon 上部署模型时，或者需要从 2-8 位进行灵活量化且无需 GPU 时使用。
