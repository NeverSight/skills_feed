Ottimizza l'inferenza LLM con NVIDIA TensorRT per il massimo throughput e la latenza più bassa. Utilizzalo per la distribuzione in produzione su GPU NVIDIA (A100/H100), quando hai bisogno di un'inferenza 10-100 volte più veloce rispetto a PyTorch o per servire modelli con quantizzazione (FP8/INT4), batching in volo e ridimensionamento multi-GPU.
