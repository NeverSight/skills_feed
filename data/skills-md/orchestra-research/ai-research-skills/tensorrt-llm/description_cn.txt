使用 NVIDIA TensorRT 优化 LLM 推理，以实现最大吞吐量和最低延迟。当您需要比 PyTorch 快 10-100 倍的推理速度时，可用于 NVIDIA GPU (A100/H100) 上的生产部署，或者用于通过量化 (FP8/INT4)、动态批处理和多 GPU 扩展来服务模型。
