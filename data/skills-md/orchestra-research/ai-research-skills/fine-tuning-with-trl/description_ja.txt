TRL による強化学習を使用して LLM を微調整します。命令調整には SFT、好みの調整には DPO、報酬の最適化には PPO/GRPO、報酬モデルのトレーニングに使用します。 RLHF が必要な場合に使用したり、モデルを好みに合わせたり、人間のフィードバックからトレーニングしたりできます。ハギングフェイストランスフォーマーと連携します。
