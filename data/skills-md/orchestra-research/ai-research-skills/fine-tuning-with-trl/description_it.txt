Ottimizza gli LLM utilizzando l'apprendimento per rinforzo con TRL - SFT per l'ottimizzazione delle istruzioni, DPO per l'allineamento delle preferenze, PPO/GRPO per l'ottimizzazione delle ricompense e la formazione sui modelli di ricompensa. Utilizza quando necessario RLHF, allinea il modello alle preferenze o addestralo in base al feedback umano. Funziona con i trasformatori HuggingFace.
