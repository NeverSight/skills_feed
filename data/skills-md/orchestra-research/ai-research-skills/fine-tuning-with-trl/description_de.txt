Optimieren Sie LLMs mithilfe von Reinforcement Learning mit TRL – SFT zur Anweisungsoptimierung, DPO zur Präferenzausrichtung, PPO/GRPO zur Belohnungsoptimierung und Belohnungsmodelltraining. Verwenden Sie bei Bedarf RLHF, richten Sie das Modell an Präferenzen aus oder trainieren Sie anhand von menschlichem Feedback. Funktioniert mit HuggingFace Transformers.
