Affinez les LLM à l'aide de l'apprentissage par renforcement avec TRL - SFT pour le réglage des instructions, DPO pour l'alignement des préférences, PPO/GRPO pour l'optimisation des récompenses et la formation du modèle de récompense. Utilisez en cas de besoin RLHF, alignez le modèle avec les préférences ou entraînez-vous à partir des commentaires humains. Fonctionne avec les transformateurs HuggingFace.
