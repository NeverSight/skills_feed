Orchestration de formation distribuée entre les clusters. Fait évoluer PyTorch/TensorFlow/HuggingFace d'un ordinateur portable à des milliers de nœuds. Réglage des hyperparamètres intégré avec Ray Tune, tolérance aux pannes, mise à l'échelle élastique. À utiliser lors de la formation de modèles massifs sur plusieurs machines ou lors de l’exécution de balayages d’hyperparamètres distribués.
