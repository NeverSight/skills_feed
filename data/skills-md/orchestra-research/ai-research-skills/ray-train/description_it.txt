Orchestrazione della formazione distribuita tra i cluster. Scala PyTorch/TensorFlow/HuggingFace dal laptop a migliaia di nodi. Ottimizzazione degli iperparametri integrata con Ray Tune, tolleranza agli errori, ridimensionamento elastico. Da utilizzare durante il training di modelli di grandi dimensioni su pi√π macchine o l'esecuzione di sweep di iperparametri distribuiti.
