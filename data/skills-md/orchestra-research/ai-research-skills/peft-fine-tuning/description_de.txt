Parametereffiziente Feinabstimmung für LLMs mit LoRA, QLoRA und über 25 Methoden. Verwenden Sie diese Option bei der Feinabstimmung großer Modelle (7B-70B) mit begrenztem GPU-Speicher, wenn Sie <1 % der Parameter mit minimalem Genauigkeitsverlust trainieren müssen, oder für die Bereitstellung mit mehreren Adaptern. Die offizielle Bibliothek von HuggingFace ist in das Transformers-Ökosystem integriert.
