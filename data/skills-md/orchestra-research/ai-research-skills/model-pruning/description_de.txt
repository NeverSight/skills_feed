Reduzieren Sie die LLM-Größe und beschleunigen Sie die Inferenz mithilfe von Beschneidungstechniken wie Wanda und SparseGPT. Verwenden Sie diese Option, wenn Sie Modelle ohne erneutes Training komprimieren, eine Sparsität von 50 % mit minimalem Genauigkeitsverlust erreichen oder eine schnellere Inferenz auf Hardwarebeschleunigern ermöglichen möchten. Behandelt unstrukturiertes Pruning, strukturiertes Pruning, N:M-Sparsity, Magnitude Pruning und One-Shot-Methoden.
