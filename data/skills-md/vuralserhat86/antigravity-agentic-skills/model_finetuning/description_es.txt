Ajuste los LLM mediante el aprendizaje por refuerzo con TRL - SFT para ajustar las instrucciones, DPO para la alineación de preferencias, PPO/GRPO para la optimización de recompensas y capacitación de modelos de recompensa. Úselo cuando necesite RLHF, alinee el modelo con las preferencias o entrene a partir de comentarios humanos. Funciona con transformadores HuggingFace.
