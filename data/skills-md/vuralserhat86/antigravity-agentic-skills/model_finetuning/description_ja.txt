TRL による強化学習を使用して LLM を微調整します。命令調整には SFT、好みの調整には DPO、報酬の最適化には PPO/GRPO、報酬モデルのトレーニングに使用します。 RLHF が必要な場合に使用し、モデルを好みに合わせて調整するか、人間のフィードバックからトレーニングします。ハギングフェイストランスフォーマーと連携します。
