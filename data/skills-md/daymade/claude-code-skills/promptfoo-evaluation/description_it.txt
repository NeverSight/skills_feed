Configura ed esegue la valutazione LLM utilizzando il framework Promptfoo. Da utilizzare durante l'impostazione dei test dei prompt, la creazione di configurazioni di valutazione (promptfooconfig.yaml), la scrittura di asserzioni personalizzate Python, l'implementazione di llm-rubric per LLM-as-judge o la gestione di esempi di poche riprese nei prompt. Si attiva su parole chiave come "promptfoo", "eval", "valutazione LLM", "test rapido" o "confronto modelli".
