Konfiguriert und führt die LLM-Bewertung mithilfe des Promptfoo-Frameworks aus. Verwenden Sie diese Option, wenn Sie Eingabeaufforderungstests einrichten, Bewertungskonfigurationen erstellen (promptfooconfig.yaml), benutzerdefinierte Python-Assertionen schreiben, llm-rubric für LLM-as-judge implementieren oder wenige Beispiele in Eingabeaufforderungen verwalten. Löst auf Schlüsselwörtern wie „promptfoo“, „eval“, „LLM-Bewertung“, „prompt testen“ oder „Modellvergleich“ aus.
