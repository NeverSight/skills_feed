Configura y ejecuta la evaluación LLM utilizando el marco Promptfoo. Úselo al configurar pruebas de avisos, crear configuraciones de evaluación (promptfooconfig.yaml), escribir aserciones personalizadas de Python, implementar llm-rubric para LLM-as-juez o administrar ejemplos breves en avisos. Activadores de palabras clave como "promptfoo", "eval", "evaluación LLM", "pruebas rápidas" o "comparación de modelos".
