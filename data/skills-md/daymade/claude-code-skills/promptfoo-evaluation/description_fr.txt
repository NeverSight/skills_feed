Configure et exécute l'évaluation LLM à l'aide du framework Promptfoo. À utiliser lors de la configuration de tests d'invite, de la création de configurations d'évaluation (promptfooconfig.yaml), de l'écriture d'assertions personnalisées Python, de l'implémentation de la rubrique llm pour LLM-as-judge ou de la gestion de quelques exemples dans les invites. Se déclenche sur des mots-clés tels que "promptfoo", "eval", "évaluation LLM", "test rapide" ou "comparaison de modèles".
