Caching strategies for LLM prompts including Anthropic prompt caching, response caching, and CAG (Cache Augmented Generation)Use when "prompt caching, cache prompt, response cache, cag, cache augmented, caching, llm, performance, optimization, cost" mentioned.
