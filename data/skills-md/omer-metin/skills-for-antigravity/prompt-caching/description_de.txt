Caching-Strategien für LLM-Eingabeaufforderungen, einschließlich Anthropic-Eingabeaufforderungs-Caching, Antwort-Caching und CAG (Cache Augmented Generation). Werden verwendet, wenn „Prompt-Caching, Cache-Eingabeaufforderung, Antwort-Cache, Cag, erweiterter Cache, Caching, LLM, Leistung, Optimierung, Kosten“ erwähnt wird.
