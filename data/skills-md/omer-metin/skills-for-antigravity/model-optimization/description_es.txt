Úselo al reducir el tamaño del modelo, mejorar la velocidad de inferencia o implementar en dispositivos de borde: cubre cuantificación, poda, destilación de conocimientos, exportación ONNX y optimización de TensorRT. Úselo cuando ", " se mencione.
