Da utilizzare durante l'implementazione di meccanismi di attenzione, la creazione di modelli di trasformatori personalizzati, la comprensione della codifica posizionale o l'ottimizzazione dell'inferenza del trasformatore: copre l'autoattenzione, l'attenzione multi-testa, RoPE, ALiBi e le varianti dell'architettura. Utilizzare quando "," menzionato.
