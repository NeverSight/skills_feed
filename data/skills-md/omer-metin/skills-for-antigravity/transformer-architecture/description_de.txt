Wird verwendet, wenn Aufmerksamkeitsmechanismen implementiert, benutzerdefinierte Transformatormodelle erstellt, Positionskodierung verstanden oder Transformatorinferenz optimiert werden – deckt Selbstaufmerksamkeit, Mehrkopfaufmerksamkeit, RoPE, ALiBi und Architekturvarianten ab. Verwenden Sie, wenn „“, „ erwähnt wird.
