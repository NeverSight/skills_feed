À utiliser lors de la mise en œuvre de mécanismes d'attention, de la création de modèles de transformateur personnalisés, de la compréhension du codage de position ou de l'optimisation de l'inférence du transformateur - couvre l'auto-attention, l'attention multi-têtes, RoPE, ALiBi et les variantes d'architecture. Utiliser lorsque ", " est mentionné.
