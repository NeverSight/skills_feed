Úselo al implementar mecanismos de atención, crear modelos de transformadores personalizados, comprender la codificación posicional u optimizar la inferencia del transformador: cubre la autoatención, la atención de múltiples cabezales, RoPE, ALiBi y variantes de arquitectura. Úselo cuando ", " se mencione.
