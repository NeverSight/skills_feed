Optimice los costos y la latencia de LLM mediante el almacenamiento en caché de KV y el almacenamiento en caché rápido. Úselo cuando (1) estructurar solicitudes de aciertos de caché, (2) configurar API cache_control para Anthropic/Cohere/OpenAI/Gemini, (3) configurar inferencia autohospedada con vLLM/SGLang/Ollama, (4) crear flujos de trabajo agentes con reutilización de prefijos, (5) diseñar canalizaciones de procesamiento por lotes o (6) comprender los precios de caché y las compensaciones.
