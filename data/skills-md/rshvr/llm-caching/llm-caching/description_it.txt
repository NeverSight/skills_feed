Ottimizza i costi e la latenza LLM tramite la memorizzazione nella cache KV e la memorizzazione nella cache tempestiva. Da utilizzare quando (1) si strutturano richieste per gli hit della cache, (2) si configura l'API cache_control per Anthropic/Cohere/OpenAI/Gemini, (3) si configura l'inferenza self-hosted con vLLM/SGLang/Ollama, (4) si creano flussi di lavoro agenti con il riutilizzo del prefisso, (5) si progettano pipeline di elaborazione batch o (6) si comprendono i prezzi e i compromessi della cache.
