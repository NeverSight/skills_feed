Optimisez les coûts et la latence LLM grâce à la mise en cache KV et à la mise en cache rapide. À utiliser lorsque (1) la structuration des invites pour les accès au cache, (2) la configuration de l'API cache_control pour Anthropic/Cohere/OpenAI/Gemini, (3) la configuration de l'inférence auto-hébergée avec vLLM/SGLang/Ollama, (4) la création de workflows agents avec réutilisation de préfixes, (5) la conception de pipelines de traitement par lots ou (6) la compréhension des prix et des compromis du cache.
